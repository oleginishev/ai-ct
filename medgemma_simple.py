#!/usr/bin/env python3
"""
–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ MedGemma –≤ Docker
"""

import os
import torch
from transformers import AutoProcessor, AutoModelForImageTextToText

MODEL_PATH = "/app/models/medgemma_4b"

def main():
    """–ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞"""
    print('üè• MedGemma 4B - –ü—Ä–æ—Å—Ç–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞')
    print('=' * 50)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–æ–¥–µ–ª—å
    if not os.path.exists(MODEL_PATH):
        print(f'‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {MODEL_PATH}')
        return
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f'üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}')
    
    if device == "cuda":
        print(f'‚úÖ GPU: {torch.cuda.get_device_name(0)}')
        print(f'üíæ –ü–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        print('üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å...')
        processor = AutoProcessor.from_pretrained(MODEL_PATH)
        model = AutoModelForImageTextToText.from_pretrained(
            MODEL_PATH,
            dtype=torch.float16,
            device_map="auto" if device == "cuda" else None,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        if device == "cpu":
            model = model.to(device)
        
        print('‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!')
        
        # –ü—Ä–æ—Å—Ç–æ–π —Ç–µ—Å—Ç
        print('\nüß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é...')
        messages = [
            {
                "role": "user",
                "content": [{"type": "text", "text": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?"}]
            }
        ]
        
        inputs = processor.apply_chat_template(
            messages, add_generation_prompt=True, tokenize=True,
            return_dict=True, return_tensors="pt"
        )
        
        if device == "cuda":
            inputs = {k: v.to(device) for k, v in inputs.items()}
        
        input_len = inputs["input_ids"].shape[-1]
        
        with torch.inference_mode():
            generation = model.generate(
                **inputs,
                max_new_tokens=50,
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id
            )
            generation = generation[0][input_len:]
        
        result = processor.decode(generation, skip_special_tokens=True)
        
        print(f'\nüìã –†–ï–ó–£–õ–¨–¢–ê–¢:')
        print('=' * 40)
        print(result)
        print('=' * 40)
        
        print('\nüéâ MedGemma —Ä–∞–±–æ—Ç–∞–µ—Ç!')
        
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞: {e}')
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()
