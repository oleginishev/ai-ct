#!/usr/bin/env python3
"""
–û—á–µ–Ω—å –ª–µ–≥–∫–∞—è –≤–µ—Ä—Å–∏—è MedGemma –¥–ª—è MacBook Air M3
–¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time

MODEL_PATH = "./models/medgemma_4b"

def load_light_model_m3():
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤–æ–π —á–∞—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
    print('üçé –ó–∞–≥—Ä—É–∂–∞–µ–º –ª–µ–≥–∫—É—é –≤–µ—Ä—Å–∏—é –¥–ª—è M3...')
    
    try:
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å (–±–µ–∑ vision)
        device = "mps" if torch.backends.mps.is_available() else "cpu"
        print(f'üñ•Ô∏è  –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}')
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ç–æ–ª—å–∫–æ —Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä –∏ —Ç–µ–∫—Å—Ç–æ–≤—É—é –º–æ–¥–µ–ª—å
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            device_map=None,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        model = model.to(device)
        
        print('‚úÖ –õ–µ–≥–∫–∞—è –º–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞!')
        return model, tokenizer, device
        
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞: {e}')
        return None, None, None

def fast_medical_qa(model, tokenizer, device, question):
    """–û—á–µ–Ω—å –±—ã—Å—Ç—Ä—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã"""
    print(f'\n‚ö° –ë—ã—Å—Ç—Ä—ã–π –≤–æ–ø—Ä–æ—Å: {question}')
    
    start_time = time.time()
    
    try:
        # –û—á–µ–Ω—å –ø—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        prompt = f"–ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å: {question}\n–û—Ç–≤–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:"
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # –ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=40,  # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç
                do_sample=False,
                temperature=0.1,
                pad_token_id=tokenizer.eos_token_id,
                use_cache=True
            )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        answer = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f'‚ö° –û–¢–í–ï–¢ (–∑–∞ {processing_time:.1f} —Å–µ–∫):')
        print('=' * 40)
        print(answer)
        print('=' * 40)
        
        return answer, processing_time
        
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞: {e}')
        return None, 0

def interactive_qa():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –≤–æ–ø—Ä–æ—Å–æ–≤"""
    print('\nüéØ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤')
    print('–í–≤–µ–¥–∏—Ç–µ "quit" –¥–ª—è –≤—ã—Ö–æ–¥–∞')
    
    model, tokenizer, device = load_light_model_m3()
    if model is None:
        return
    
    while True:
        question = input('\n‚ùì –í–∞—à –≤–æ–ø—Ä–æ—Å: ').strip()
        
        if question.lower() in ['quit', 'exit', '–≤—ã—Ö–æ–¥']:
            break
        
        if not question:
            continue
        
        fast_medical_qa(model, tokenizer, device, question)

def quick_demo():
    """–ë—ã—Å—Ç—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è"""
    print('üöÄ –ë—ã—Å—Ç—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –¥–ª—è M3...')
    
    model, tokenizer, device = load_light_model_m3()
    if model is None:
        return
    
    # –ë—ã—Å—Ç—Ä—ã–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º
    questions = [
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –ø–Ω–µ–≤–º–æ–Ω–∏—è?",
        "–°–∏–º–ø—Ç–æ–º—ã –≥—Ä–∏–ø–ø–∞?",
        "–ö–∞–∫ –ª–µ—á–∏—Ç—å —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –¥–∏–∞–±–µ—Ç?",
        "–ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–Ω—Ñ–∞—Ä–∫—Ç–∞?"
    ]
    
    times = []
    for i, q in enumerate(questions, 1):
        print(f'\n--- –í–æ–ø—Ä–æ—Å {i}/{len(questions)} ---')
        answer, time_taken = fast_medical_qa(model, tokenizer, device, q)
        if time_taken > 0:
            times.append(time_taken)
    
    if times:
        avg_time = sum(times) / len(times)
        print(f'\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:')
        print(f'   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.1f} —Å–µ–∫')
        print(f'   –í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(times)}')
        
        if avg_time < 5:
            print('‚ö° –û—Ç–ª–∏—á–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å!')
        elif avg_time < 15:
            print('‚úÖ –•–æ—Ä–æ—à–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å')
        else:
            print('üêå –ú–µ–¥–ª–µ–Ω–Ω–æ –¥–ª—è M3')

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print('üçé MedGemma Light - MacBook Air M3')
    print('=' * 50)
    print('üí° –¢–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç–æ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –¥–ª—è –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏')
    
    if not os.path.exists(MODEL_PATH):
        print(f'‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {MODEL_PATH}')
        return
    
    print('\n–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:')
    print('1. –ë—ã—Å—Ç—Ä–∞—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è')
    print('2. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–µ –≤–æ–ø—Ä–æ—Å—ã')
    
    choice = input('\n–í–∞—à –≤—ã–±–æ—Ä (1/2): ').strip()
    
    if choice == '1':
        quick_demo()
    elif choice == '2':
        interactive_qa()
    else:
        print('–ó–∞–ø—É—Å–∫–∞–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é...')
        quick_demo()

if __name__ == "__main__":
    main()
