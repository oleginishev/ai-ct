#!/usr/bin/env python3
"""
MedGemma –¥–ª—è Docker —Å GPU –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π
"""

import os
from transformers import AutoProcessor, AutoModelForImageTextToText
from PIL import Image
import torch
import requests

# –ü—É—Ç—å –∫ –º–æ–¥–µ–ª–∏ –≤ Docker –∫–æ–Ω—Ç–µ–π–Ω–µ—Ä–µ
MODEL_PATH = "/app/models/medgemma_4b"

def load_medgemma_gpu():
    """–ó–∞–≥—Ä—É–∑–∫–∞ MedGemma —Å GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π"""
    print('üîÑ –ó–∞–≥—Ä—É–∂–∞–µ–º MedGemma —Å GPU...')
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU
        if torch.cuda.is_available():
            device = "cuda"
            print(f'‚úÖ CUDA –¥–æ—Å—Ç—É–ø–Ω–∞: {torch.cuda.get_device_name(0)}')
            print(f'üíæ GPU –ø–∞–º—è—Ç—å: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB')
        else:
            device = "cpu"
            print('‚ö†Ô∏è  CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞, –∏—Å–ø–æ–ª—å–∑—É–µ–º CPU')
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –ø—Ä–æ—Ü–µ—Å—Å–æ—Ä
        processor = AutoProcessor.from_pretrained(MODEL_PATH)
        print('‚úÖ Processor –∑–∞–≥—Ä—É–∂–µ–Ω!')
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å —Å GPU –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π
        model = AutoModelForImageTextToText.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,  # float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
            device_map="auto" if device == "cuda" else None,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        if device == "cpu":
            model = model.to(device)
        
        print(f'‚úÖ Model –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}!')
        print('üéâ MedGemma —Å GPU –≥–æ—Ç–æ–≤–∞ –∫ —Ä–∞–±–æ—Ç–µ!')
        
        return model, processor, device
        
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}')
        return None, None, None

def analyze_chest_xray_gpu(model, processor, device, image_path=None):
    """–ê–Ω–∞–ª–∏–∑ —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–æ–≥–æ —Å–Ω–∏–º–∫–∞ —Å GPU"""
    print('\nü´Å –ê–Ω–∞–ª–∏–∑ —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–æ–≥–æ —Å–Ω–∏–º–∫–∞ (GPU)...')
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if image_path and os.path.exists(image_path):
            image = Image.open(image_path).convert('RGB')
            print(f'üìÅ –ó–∞–≥—Ä—É–∂–µ–Ω–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}')
        else:
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            image_url = "https://upload.wikimedia.org/wikipedia/commons/c/c8/Chest_Xray_PA_3-8-2010.png"
            image = Image.open(requests.get(image_url, headers={"User-Agent": "example"}, stream=True).raw)
            print('üì∑ –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø—Ä–∏–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –∏–∑ –∏–Ω—Ç–µ—Ä–Ω–µ—Ç–∞')
        
        # –§–æ—Ä–º–∏—Ä—É–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        messages = [
            {
                "role": "system",
                "content": [{"type": "text", "text": "–í—ã —ç–∫—Å–ø–µ—Ä—Ç-—Ä–µ–Ω—Ç–≥–µ–Ω–æ–ª–æ–≥. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–∏–π —Å–Ω–∏–º–æ–∫ –≥—Ä—É–¥–Ω–æ–π –∫–ª–µ—Ç–∫–∏ –∏ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤—å—Ç–µ –ø–æ–¥—Ä–æ–±–Ω—ã–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –æ—Ç—á–µ—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."}]
            },
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π—Ç–µ —ç—Ç–æ—Ç —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–∏–π —Å–Ω–∏–º–æ–∫ –≥—Ä—É–¥–Ω–æ–π –∫–ª–µ—Ç–∫–∏ –∏ –æ–ø–∏—à–∏—Ç–µ –ª—é–±—ã–µ –Ω–∞—Ö–æ–¥–∫–∏, –∞–Ω–æ–º–∞–ª–∏–∏ –∏–ª–∏ –Ω–æ—Ä–º–∞–ª—å–Ω—ã–µ —Å—Ç—Ä—É–∫—Ç—É—Ä—ã, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã –Ω–∞–±–ª—é–¥–∞–µ—Ç–µ. –û—Ç–≤–µ—á–∞–π—Ç–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."},
                    {"type": "image", "image": image}
                ]
            }
        ]
        
        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Å GPU
        print('üîç –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ GPU...')
        inputs = processor.apply_chat_template(
            messages, add_generation_prompt=True, tokenize=True,
            return_dict=True, return_tensors="pt"
        )
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU
        if device == "cuda":
            inputs = {k: v.to(device) for k, v in inputs.items()}
        
        input_len = inputs["input_ids"].shape[-1]
        
        with torch.inference_mode():
            generation = model.generate(
                **inputs, 
                max_new_tokens=200,  # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è GPU
                do_sample=False,
                temperature=0.1,
                top_p=0.9,
                pad_token_id=processor.tokenizer.eos_token_id,
                use_cache=True
            )
            generation = generation[0][input_len:]
        
        result = processor.decode(generation, skip_special_tokens=True)
        
        print('\nüìã –†–ï–ó–£–õ–¨–¢–ê–¢ –ê–ù–ê–õ–ò–ó–ê (GPU):')
        print('=' * 60)
        print(result)
        print('=' * 60)
        
        return result
        
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ: {e}')
        return None

def medical_qa_gpu(model, processor, device, question):
    """–û—Ç–≤–µ—Ç –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å —Å GPU"""
    print(f'\n‚ùì –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å: {question}')
    
    try:
        # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º
        prompt = f"""–í—ã —ç–∫—Å–ø–µ—Ä—Ç-–º–µ–¥–∏–∫. –ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ —Å–ª–µ–¥—É—é—â–∏–π –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å —Ç–æ—á–Ω–æ –∏ –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç:"""
        
        # –¢–æ–∫–µ–Ω–∏–∑–∏—Ä—É–µ–º
        inputs = processor.tokenizer(prompt, return_tensors="pt")
        
        # –ü–µ—Ä–µ–º–µ—â–∞–µ–º –Ω–∞ GPU
        if device == "cuda":
            inputs = {k: v.to(device) for k, v in inputs.items()}
        
        print('üîç –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –≤–æ–ø—Ä–æ—Å –Ω–∞ GPU...')
        with torch.inference_mode():
            generation = model.generate(
                **inputs, 
                max_new_tokens=150,  # –ë–æ–ª—å—à–µ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è GPU
                do_sample=False,
                temperature=0.1,
                top_p=0.9,
                pad_token_id=processor.tokenizer.eos_token_id,
                use_cache=True
            )
            generation = generation[0][inputs["input_ids"].shape[-1]:]
        
        result = processor.tokenizer.decode(generation, skip_special_tokens=True)
        
        print('\nüìã –û–¢–í–ï–¢ (GPU):')
        print('=' * 60)
        print(result)
        print('=' * 60)
        
        return result
        
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –≤–æ–ø—Ä–æ—Å–∞: {e}')
        return None

def test_gpu_model():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å GPU"""
    print('üß™ –¢–µ—Å—Ç–∏—Ä—É–µ–º MedGemma —Å GPU...')
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
    model, processor, device = load_medgemma_gpu()
    if model is None or processor is None:
        return False
    
    # –¢–µ—Å—Ç 1: –ê–Ω–∞–ª–∏–∑ —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–æ–≥–æ —Å–Ω–∏–º–∫–∞
    print('\n' + '=' * 60)
    print('üìã –¢–ï–°–¢ 1: –ê–Ω–∞–ª–∏–∑ —Ä–µ–Ω—Ç–≥–µ–Ω–æ–≤—Å–∫–æ–≥–æ —Å–Ω–∏–º–∫–∞')
    analyze_chest_xray_gpu(model, processor, device)
    
    # –¢–µ—Å—Ç 2: –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å
    print('\n' + '=' * 60)
    print('üìã –¢–ï–°–¢ 2: –ú–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å')
    medical_qa_gpu(model, processor, device, "–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–∏–º–ø—Ç–æ–º—ã –ø–Ω–µ–≤–º–æ–Ω–∏–∏?")
    
    # –¢–µ—Å—Ç 3: –ï—â–µ –æ–¥–∏–Ω –≤–æ–ø—Ä–æ—Å
    print('\n' + '=' * 60)
    print('üìã –¢–ï–°–¢ 3: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã–π –≤–æ–ø—Ä–æ—Å')
    medical_qa_gpu(model, processor, device, "–ö–∞–∫ –¥–∏–∞–≥–Ω–æ—Å—Ç–∏—Ä—É–µ—Ç—Å—è —Ç—É–±–µ—Ä–∫—É–ª–µ–∑?")
    
    print('\n' + '=' * 60)
    print('‚úÖ –í—Å–µ —Ç–µ—Å—Ç—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã!')
    print('üí° MedGemma —Å GPU —Ä–∞–±–æ—Ç–∞–µ—Ç –æ—Ç–ª–∏—á–Ω–æ!')
    
    return True

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print('üè• MedGemma 4B - Docker —Å GPU')
    print('=' * 60)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–∏
    if not os.path.exists(MODEL_PATH):
        print(f'‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {MODEL_PATH}')
        print('üí° –£–±–µ–¥–∏—Ç–µ—Å—å —á—Ç–æ –ø–∞–ø–∫–∞ models —Å–º–æ–Ω—Ç–∏—Ä–æ–≤–∞–Ω–∞ –≤ Docker')
        return
    
    print(f'üìÅ –ò—Å–ø–æ–ª—å–∑—É–µ–º –º–æ–¥–µ–ª—å –∏–∑: {MODEL_PATH}')
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Ç–µ—Å—Ç—ã
    success = test_gpu_model()
    
    if success:
        print('\nüéâ MedGemma —Å GPU –≥–æ—Ç–æ–≤–∞ –∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—é!')
        print('üí° –¢–µ–ø–µ—Ä—å –≤—Å–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –±—ã—Å—Ç—Ä–æ —Å GPU —É—Å–∫–æ—Ä–µ–Ω–∏–µ–º!')
    else:
        print('\n‚ùå –í–æ–∑–Ω–∏–∫–ª–∏ –ø—Ä–æ–±–ª–µ–º—ã —Å –º–æ–¥–µ–ª—å—é')

if __name__ == "__main__":
    main()
