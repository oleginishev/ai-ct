#!/usr/bin/env python3
"""
MedGemma —Å —Ä—É—Å—Å–∫–∏–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏ - –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≤–µ—Ä—Å–∏—è –¥–ª—è MacBook Air M3
"""

import os
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
import time

MODEL_PATH = "./models/medgemma_4b"

def load_model_russian():
    """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤"""
    print('üá∑üá∫ –ó–∞–≥—Ä—É–∂–∞–µ–º MedGemma –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤...')
    
    try:
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ
        if torch.backends.mps.is_available():
            device = "mps"
            print('‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º MPS (Metal) —É—Å–∫–æ—Ä–µ–Ω–∏–µ')
        else:
            device = "cpu"
            print('‚ö†Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º CPU')
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
        tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
        model = AutoModelForCausalLM.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16,
            device_map=None,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        model = model.to(device)
        
        print(f'‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –Ω–∞ {device}')
        return model, tokenizer, device
        
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞: {e}')
        return None, None, None

def ask_medical_question(model, tokenizer, device, question):
    """–ó–∞–¥–∞—Ç—å –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –Ω–∞ —Ä—É—Å—Å–∫–æ–º"""
    print(f'\n‚ùì –í–æ–ø—Ä–æ—Å: {question}')
    
    start_time = time.time()
    
    try:
        # –ü—Ä–æ–º–ø—Ç –¥–ª—è —Ä—É—Å—Å–∫–∏—Ö –æ—Ç–≤–µ—Ç–æ–≤
        prompt = f"""–í—ã –æ–ø—ã—Ç–Ω—ã–π –≤—Ä–∞—á-—ç–∫—Å–ø–µ—Ä—Ç. –û—Ç–≤–µ—Ç—å—Ç–µ –Ω–∞ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å –ø—Ä–æ—Ñ–µ—Å—Å–∏–æ–Ω–∞–ª—å–Ω–æ –∏ –ø–æ–Ω—è—Ç–Ω–æ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.

–í–æ–ø—Ä–æ—Å: {question}

–û—Ç–≤–µ—Ç –≤—Ä–∞—á–∞:"""
        
        # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
        inputs = tokenizer(prompt, return_tensors="pt").to(device)
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞
        with torch.inference_mode():
            outputs = model.generate(
                **inputs,
                max_new_tokens=80,  # –î–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–ª—è —Ä–∞–∑–≤–µ—Ä–Ω—É—Ç–æ–≥–æ –æ—Ç–≤–µ—Ç–∞
                do_sample=False,
                temperature=0.1,
                pad_token_id=tokenizer.eos_token_id,
                use_cache=True
            )
        
        # –î–µ–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
        answer = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f'‚ö° –û–¢–í–ï–¢ (–∑–∞ {processing_time:.1f} —Å–µ–∫):')
        print('=' * 60)
        print(answer)
        print('=' * 60)
        
        return answer, processing_time
        
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞: {e}')
        return None, 0

def interactive_russian_qa():
    """–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –Ω–∞ —Ä—É—Å—Å–∫–æ–º"""
    print('\nüá∑üá∫ –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º')
    print('–í–≤–µ–¥–∏—Ç–µ "–≤—ã—Ö–æ–¥" –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è')
    
    model, tokenizer, device = load_model_russian()
    if model is None:
        return
    
    print('\nüí° –ü—Ä–∏–º–µ—Ä—ã –≤–æ–ø—Ä–æ—Å–æ–≤:')
    print('   - –ö–∞–∫–∏–µ —Å–∏–º–ø—Ç–æ–º—ã —É –ø–Ω–µ–≤–º–æ–Ω–∏–∏?')
    print('   - –ö–∞–∫ –ª–µ—á–∏—Ç—å –ø—Ä–æ—Å—Ç—É–¥—É?')
    print('   - –ß—Ç–æ —Ç–∞–∫–æ–µ –≥–∏–ø–µ—Ä—Ç–æ–Ω–∏—è?')
    print('   - –ü—Ä–∏–∑–Ω–∞–∫–∏ –∏–Ω—Ñ–∞—Ä–∫—Ç–∞?')
    
    while True:
        question = input('\n‚ùì –í–∞—à –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–π –≤–æ–ø—Ä–æ—Å: ').strip()
        
        if question.lower() in ['–≤—ã—Ö–æ–¥', 'exit', 'quit', '—Å—Ç–æ–ø']:
            print('üëã –î–æ —Å–≤–∏–¥–∞–Ω–∏—è!')
            break
        
        if not question:
            continue
        
        ask_medical_question(model, tokenizer, device, question)

def demo_russian_questions():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Ä—É—Å—Å–∫–∏—Ö –≤–æ–ø—Ä–æ—Å–æ–≤"""
    print('üöÄ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è MedGemma –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ...')
    
    model, tokenizer, device = load_model_russian()
    if model is None:
        return
    
    # –ü–æ–ø—É–ª—è—Ä–Ω—ã–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –≤–æ–ø—Ä–æ—Å—ã –Ω–∞ —Ä—É—Å—Å–∫–æ–º
    questions = [
        "–ö–∞–∫–∏–µ –æ—Å–Ω–æ–≤–Ω—ã–µ —Å–∏–º–ø—Ç–æ–º—ã –ø–Ω–µ–≤–º–æ–Ω–∏–∏?",
        "–ö–∞–∫ –ª–µ—á–∏—Ç—å –ø—Ä–æ—Å—Ç—É–¥—É –≤ –¥–æ–º–∞—à–Ω–∏—Ö —É—Å–ª–æ–≤–∏—è—Ö?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –≥–∏–ø–µ—Ä—Ç–æ–Ω–∏—è –∏ –∫–∞–∫ –µ–µ –∫–æ–Ω—Ç—Ä–æ–ª–∏—Ä–æ–≤–∞—Ç—å?",
        "–ö–∞–∫–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–Ω—Ñ–∞—Ä–∫—Ç–∞ –º–∏–æ–∫–∞—Ä–¥–∞?",
        "–ö–∞–∫ –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∏–∑–º–µ—Ä—è—Ç—å –∞—Ä—Ç–µ—Ä–∏–∞–ª—å–Ω–æ–µ –¥–∞–≤–ª–µ–Ω–∏–µ?",
        "–ß—Ç–æ —Ç–∞–∫–æ–µ –¥–∏–∞–±–µ—Ç –∏ –µ–≥–æ —Ç–∏–ø—ã?",
        "–ö–∞–∫ –æ–∫–∞–∑–∞—Ç—å –ø–µ—Ä–≤—É—é –ø–æ–º–æ—â—å –ø—Ä–∏ –æ–∂–æ–≥–∞—Ö?",
        "–ö–∞–∫–∏–µ —Å–∏–º–ø—Ç–æ–º—ã –≥—Ä–∏–ø–ø–∞ –æ—Ç–ª–∏—á–∞—é—Ç –µ–≥–æ –æ—Ç –ø—Ä–æ—Å—Ç—É–¥—ã?"
    ]
    
    times = []
    for i, question in enumerate(questions, 1):
        print(f'\n--- –í–æ–ø—Ä–æ—Å {i}/{len(questions)} ---')
        answer, time_taken = ask_medical_question(model, tokenizer, device, question)
        if time_taken > 0:
            times.append(time_taken)
        
        # –ù–µ–±–æ–ª—å—à–∞—è –ø–∞—É–∑–∞ –º–µ–∂–¥—É –≤–æ–ø—Ä–æ—Å–∞–º–∏
        if i < len(questions):
            time.sleep(1)
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
    if times:
        avg_time = sum(times) / len(times)
        print(f'\nüìä –°–¢–ê–¢–ò–°–¢–ò–ö–ê:')
        print(f'   –í—Å–µ–≥–æ –≤–æ–ø—Ä–æ—Å–æ–≤: {len(times)}')
        print(f'   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.1f} —Å–µ–∫')
        print(f'   –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ: {min(times):.1f} —Å–µ–∫')
        print(f'   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ: {max(times):.1f} —Å–µ–∫')
        
        if avg_time < 8:
            print('‚ö° –û—Ç–ª–∏—á–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å!')
        elif avg_time < 20:
            print('‚úÖ –•–æ—Ä–æ—à–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å')
        else:
            print('üêå –ú–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ —Ä–∞–±–æ—Ç–∞–µ—Ç')

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print('üá∑üá∫ MedGemma - –†—É—Å—Å–∫–∏–µ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–∏–µ –æ—Ç–≤–µ—Ç—ã')
    print('=' * 60)
    print('üçé –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–æ –¥–ª—è MacBook Air M3')
    
    if not os.path.exists(MODEL_PATH):
        print(f'‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {MODEL_PATH}')
        print('üí° –°–∫–∞—á–∞–π—Ç–µ –º–æ–¥–µ–ª—å: huggingface-cli download google/medgemma-4b-it --local-dir models/medgemma_4b')
        return
    
    print('\n–í—ã–±–µ—Ä–∏—Ç–µ —Ä–µ–∂–∏–º:')
    print('1. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è (8 –ø—Ä–∏–º–µ—Ä–æ–≤ –≤–æ–ø—Ä–æ—Å–æ–≤)')
    print('2. –ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π —Ä–µ–∂–∏–º')
    
    choice = input('\n–í–∞—à –≤—ã–±–æ—Ä (1/2): ').strip()
    
    if choice == '1':
        demo_russian_questions()
    elif choice == '2':
        interactive_russian_qa()
    else:
        print('–ó–∞–ø—É—Å–∫–∞–µ–º –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—é...')
        demo_russian_questions()

if __name__ == "__main__":
    main()
