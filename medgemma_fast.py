#!/usr/bin/env python3
"""
–ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è MedGemma –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
"""

import os
import torch
from transformers import AutoProcessor, AutoModelForImageTextToText
from PIL import Image
import time

MODEL_PATH = "./models/medgemma_4b"

def load_model_fast():
    """–ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏"""
    print('‚ö° –ë—ã—Å—Ç—Ä–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ MedGemma...')
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º GPU
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f'üñ•Ô∏è  –ò—Å–ø–æ–ª—å–∑—É–µ–º —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}')
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è–º–∏
        model = AutoModelForImageTextToText.from_pretrained(
            MODEL_PATH,
            torch_dtype=torch.float16 if device == "cuda" else torch.float32,
            device_map="auto" if device == "cuda" else None,
            low_cpu_mem_usage=True,
            trust_remote_code=True
        )
        
        processor = AutoProcessor.from_pretrained(MODEL_PATH)
        
        if device == "cpu":
            model = model.to(device)
        
        print('‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –±—ã—Å—Ç—Ä–æ!')
        return model, processor, device
        
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞: {e}')
        return None, None, None

def quick_analyze(model, processor, device, image_path=None):
    """–ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è"""
    print('\n‚ö° –ë—ã—Å—Ç—Ä—ã–π –∞–Ω–∞–ª–∏–∑ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è...')
    
    start_time = time.time()
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
        if image_path and os.path.exists(image_path):
            image = Image.open(image_path).convert('RGB')
            print(f'üìÅ –ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}')
        else:
            # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ
            image = Image.new('RGB', (512, 512), color='white')
            print('üì∑ –¢–µ—Å—Ç–æ–≤–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ')
        
        # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–æ–º–ø—Ç –Ω–∞ —Ä—É—Å—Å–∫–æ–º –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "text", "text": "–ö—Ä–∞—Ç–∫–æ –æ–ø–∏—à–∏—Ç–µ —ç—Ç–æ –º–µ–¥–∏—Ü–∏–Ω—Å–∫–æ–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ."},
                    {"type": "image", "image": image}
                ]
            }
        ]
        
        # –ë—ã—Å—Ç—Ä–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
        inputs = processor.apply_chat_template(
            messages, 
            add_generation_prompt=True, 
            tokenize=True,
            return_dict=True, 
            return_tensors="pt"
        )
        
        if device == "cuda":
            inputs = {k: v.to(device) for k, v in inputs.items()}
        
        input_len = inputs["input_ids"].shape[-1]
        
        # –ë—ã—Å—Ç—Ä–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è
        with torch.inference_mode():
            generation = model.generate(
                **inputs,
                max_new_tokens=50,  # –û—á–µ–Ω—å –∫–æ—Ä–æ—Ç–∫–∏–π –æ—Ç–≤–µ—Ç –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                do_sample=False,
                pad_token_id=processor.tokenizer.eos_token_id,
                use_cache=True
            )
            generation = generation[0][input_len:]
        
        result = processor.decode(generation, skip_special_tokens=True)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        print(f'\n‚ö° –†–ï–ó–£–õ–¨–¢–ê–¢ (–∑–∞ {processing_time:.1f} —Å–µ–∫):')
        print('=' * 50)
        print(result)
        print('=' * 50)
        
        return result, processing_time
        
    except Exception as e:
        print(f'‚ùå –û—à–∏–±–∫–∞: {e}')
        return None, 0

def benchmark_model():
    """–ë–µ–Ω—á–º–∞—Ä–∫ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏"""
    print('üèÅ –ë–µ–Ω—á–º–∞—Ä–∫ MedGemma...')
    
    model, processor, device = load_model_fast()
    if model is None:
        return
    
    print(f'\nüìä –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –º–æ–¥–µ–ª–∏:')
    print(f'   –£—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}')
    print(f'   –¢–∏–ø –¥–∞–Ω–Ω—ã—Ö: {model.dtype}')
    print(f'   –ü–∞—Ä–∞–º–µ—Ç—Ä—ã: ~4B')
    
    # –¢–µ—Å—Ç —Å–∫–æ—Ä–æ—Å—Ç–∏
    times = []
    for i in range(3):
        print(f'\nüîÑ –¢–µ—Å—Ç {i+1}/3...')
        result, time_taken = quick_analyze(model, processor, device)
        if time_taken > 0:
            times.append(time_taken)
    
    if times:
        avg_time = sum(times) / len(times)
        print(f'\nüìà –†–ï–ó–£–õ–¨–¢–ê–¢–´ –ë–ï–ù–ß–ú–ê–†–ö–ê:')
        print(f'   –°—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è: {avg_time:.1f} —Å–µ–∫')
        print(f'   –ú–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ: {min(times):.1f} —Å–µ–∫')
        print(f'   –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ: {max(times):.1f} —Å–µ–∫')
        
        if avg_time < 10:
            print('‚ö° –û—Ç–ª–∏—á–Ω–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å!')
        elif avg_time < 30:
            print('‚úÖ –•–æ—Ä–æ—à–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å')
        else:
            print('üêå –ú–µ–¥–ª–µ–Ω–Ω–æ, –Ω—É–∂–Ω–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è')

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print('‚ö° MedGemma 4B - –ë—ã—Å—Ç—Ä–∞—è –≤–µ—Ä—Å–∏—è')
    print('=' * 50)
    
    if not os.path.exists(MODEL_PATH):
        print(f'‚ùå –ú–æ–¥–µ–ª—å –Ω–µ –Ω–∞–π–¥–µ–Ω–∞ –≤ {MODEL_PATH}')
        return
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º –±–µ–Ω—á–º–∞—Ä–∫
    benchmark_model()
    
    print('\nüí° –°–æ–≤–µ—Ç—ã –¥–ª—è —É—Å–∫–æ—Ä–µ–Ω–∏—è:')
    print('   1. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ GPU (CUDA)')
    print('   2. –£–º–µ–Ω—å—à–∏—Ç–µ max_new_tokens')
    print('   3. –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ torch.float16')
    print('   4. –í–∫–ª—é—á–∏—Ç–µ use_cache=True')

if __name__ == "__main__":
    main()
